# Configuration Centralization System
# All tunable parameters for RAG, LLM, and Prompt Engineering experiments

# RAG Configuration
rag:
  enabled: true
  n_results: 3
  relevance_threshold: 1.0
  collection_name: "documents"
  query_extraction:
    max_tokens: 100
    query_template: |
      Extract the main search query or key information need from this message.
      Return ONLY the essential search terms or question, removing conversational elements.
      Be concise - return 1-2 sentences maximum.
      
      Message: {prompt}
      
      Search query:

prompts:
  system_message: "You are a helpful assistant. Provide the user with coherent answer to their question."
  context_template: |
    === Additional Contextual Information ===
    {context}
    
    === User Question ===
    {content}
    
    Provide a coherent answer to the user question based on:
    * The additional contextual information
    * Your general knowledge
    * Use termology relevant to the context
    * Point out incorrect assumptions

  academic_context_header: "=== Academic Research Papers from arXiv ==="


# LLM Configuration
llm:
  default_model: "qwen3:30b"
  timeouts:
    streaming: 120.0      # Streaming chat requests (longer responses)
    completion: 30.0      # Non-streaming completions (quick tasks)

# Academic Search Configuration
academic_search:
  enabled: false
  max_results: 5         # Number of papers to retrieve from arXiv

# Document Processing Configuration
document_processing:
  chunking:
    max_chars: 1500      # Chunk size for document splitting
    overlap: 0           # Future: add overlap between chunks
  supported_formats: [".pdf", ".txt"]
  batch_size: 50         # Batch size for vector DB ingestion

# Vector Database Configuration
vectordb:
  timeout: 60              # HTTP request timeout for vector DB operations

# Service URLs (can be overridden by environment variables)
services:
  llm_url: "http://api:3001"
  vectordb_url: "http://host.docker.internal:8003"

# Logging Configuration
logging:
  level: "INFO"
